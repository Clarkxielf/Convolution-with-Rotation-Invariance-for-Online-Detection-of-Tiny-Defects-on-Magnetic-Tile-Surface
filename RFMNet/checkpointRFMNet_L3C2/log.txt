save path : ./checkpointRFMNet_L3C2
Namespace(arch='rfmnet', batch_size=32, data='../Data_processing_data2/Result_Split_data', epochs=30, evaluate=False, lr=0.001, momentum=0.9, pin_memory=True, print_freq=20, resume='', save_dir='./checkpointRFMNet_L3C2', start_epoch=0, weight_decay=0.0001, workers=4)
=> creating model 'rfmnet'
=> network :
 RFMNet(
  (conv1): Conv2d(1, 128, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): RFMBlock(
      (conv1): Conv2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
      (downsample): Conv2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)
      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer2): Sequential(
    (0): RFMBlock(
      (conv1): Conv2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
      (downsample): Conv2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False)
      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): RFMBlock(
      (conv1): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): RFMBlock(
      (conv1): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer3): Sequential(
    (0): RFMBlock(
      (conv1): Conv2d(512, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256, bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)
      (downsample): Conv2d(512, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256, bias=False)
      (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): RFMBlock(
      (conv1): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)
      (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): RFMBlock(
      (conv1): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)
      (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(2, 2))
  (fc): Linear(in_features=4096, out_features=3, bias=True)
)
Epoch: [0][0/188]	Time 0.800 (0.800)	Data 0.404 (0.404)	Loss 1.0272 (1.0272)	Prec@1 46.875 (46.875)	
Epoch: [0][20/188]	Time 0.071 (0.107)	Data 0.000 (0.019)	Loss 1.0436 (1.0895)	Prec@1 40.625 (37.500)	
Epoch: [0][40/188]	Time 0.071 (0.090)	Data 0.000 (0.010)	Loss 1.0049 (1.0561)	Prec@1 53.125 (44.436)	
Epoch: [0][60/188]	Time 0.068 (0.084)	Data 0.000 (0.007)	Loss 1.0786 (1.0114)	Prec@1 59.375 (48.975)	
Epoch: [0][80/188]	Time 0.067 (0.081)	Data 0.000 (0.005)	Loss 0.8530 (0.9849)	Prec@1 68.750 (51.196)	
Epoch: [0][100/188]	Time 0.079 (0.078)	Data 0.000 (0.004)	Loss 1.0044 (0.9672)	Prec@1 43.750 (52.228)	
Epoch: [0][120/188]	Time 0.074 (0.077)	Data 0.000 (0.003)	Loss 0.7065 (0.9552)	Prec@1 59.375 (53.177)	
Epoch: [0][140/188]	Time 0.074 (0.076)	Data 0.000 (0.003)	Loss 0.8506 (0.9460)	Prec@1 65.625 (53.989)	
Epoch: [0][160/188]	Time 0.070 (0.076)	Data 0.000 (0.003)	Loss 0.7439 (0.9371)	Prec@1 62.500 (54.717)	
Epoch: [0][180/188]	Time 0.067 (0.075)	Data 0.000 (0.002)	Loss 0.7361 (0.9275)	Prec@1 68.750 (55.421)	
Test: [0/8]	Time 0.258 (0.258)	Loss 0.5365 (0.5365)	Prec@1 78.125 (78.125)	
 * Prec@1 76.991
Epoch: [1][0/188]	Time 0.309 (0.309)	Data 0.237 (0.237)	Loss 1.1293 (1.1293)	Prec@1 50.000 (50.000)	
Epoch: [1][20/188]	Time 0.068 (0.084)	Data 0.000 (0.011)	Loss 0.7259 (0.9232)	Prec@1 75.000 (58.333)	
Epoch: [1][40/188]	Time 0.073 (0.075)	Data 0.000 (0.006)	Loss 0.6783 (0.8486)	Prec@1 68.750 (61.357)	
Epoch: [1][60/188]	Time 0.060 (0.072)	Data 0.000 (0.004)	Loss 0.6883 (0.8286)	Prec@1 65.625 (62.090)	
Epoch: [1][80/188]	Time 0.067 (0.070)	Data 0.000 (0.003)	Loss 0.7026 (0.8061)	Prec@1 56.250 (63.503)	
Epoch: [1][100/188]	Time 0.068 (0.070)	Data 0.000 (0.002)	Loss 0.6311 (0.7889)	Prec@1 78.125 (64.171)	
Epoch: [1][120/188]	Time 0.064 (0.070)	Data 0.000 (0.002)	Loss 1.1603 (0.7903)	Prec@1 59.375 (64.463)	
Epoch: [1][140/188]	Time 0.074 (0.070)	Data 0.000 (0.002)	Loss 0.7551 (0.7827)	Prec@1 68.750 (64.805)	
Epoch: [1][160/188]	Time 0.070 (0.070)	Data 0.000 (0.002)	Loss 0.7690 (0.7763)	Prec@1 75.000 (65.140)	
Epoch: [1][180/188]	Time 0.071 (0.070)	Data 0.000 (0.001)	Loss 0.9439 (0.7728)	Prec@1 56.250 (65.073)	
Test: [0/8]	Time 0.267 (0.267)	Loss 0.5422 (0.5422)	Prec@1 68.750 (68.750)	
 * Prec@1 83.628
Epoch: [2][0/188]	Time 0.296 (0.296)	Data 0.221 (0.221)	Loss 0.7712 (0.7712)	Prec@1 68.750 (68.750)	
Epoch: [2][20/188]	Time 0.064 (0.082)	Data 0.000 (0.011)	Loss 0.7048 (0.7363)	Prec@1 68.750 (66.369)	
Epoch: [2][40/188]	Time 0.073 (0.077)	Data 0.000 (0.006)	Loss 0.6385 (0.7050)	Prec@1 65.625 (67.988)	
Epoch: [2][60/188]	Time 0.071 (0.075)	Data 0.000 (0.004)	Loss 0.6667 (0.7028)	Prec@1 71.875 (68.289)	
Epoch: [2][80/188]	Time 0.078 (0.074)	Data 0.000 (0.003)	Loss 0.8618 (0.7084)	Prec@1 62.500 (67.785)	
Epoch: [2][100/188]	Time 0.076 (0.073)	Data 0.000 (0.002)	Loss 0.8352 (0.7142)	Prec@1 68.750 (67.946)	
Epoch: [2][120/188]	Time 0.068 (0.073)	Data 0.000 (0.002)	Loss 0.6882 (0.7127)	Prec@1 75.000 (68.001)	
Epoch: [2][140/188]	Time 0.070 (0.073)	Data 0.000 (0.002)	Loss 0.7324 (0.7041)	Prec@1 65.625 (68.772)	
Epoch: [2][160/188]	Time 0.073 (0.073)	Data 0.000 (0.002)	Loss 0.6332 (0.6960)	Prec@1 71.875 (69.216)	
Epoch: [2][180/188]	Time 0.071 (0.073)	Data 0.000 (0.001)	Loss 0.8181 (0.6954)	Prec@1 71.875 (69.372)	
Test: [0/8]	Time 0.264 (0.264)	Loss 0.7824 (0.7824)	Prec@1 75.000 (75.000)	
 * Prec@1 88.053
Epoch: [3][0/188]	Time 0.301 (0.301)	Data 0.213 (0.213)	Loss 0.5007 (0.5007)	Prec@1 84.375 (84.375)	
Epoch: [3][20/188]	Time 0.074 (0.083)	Data 0.000 (0.010)	Loss 0.5052 (0.6477)	Prec@1 81.250 (71.577)	
Epoch: [3][40/188]	Time 0.075 (0.077)	Data 0.000 (0.005)	Loss 0.5931 (0.6422)	Prec@1 75.000 (72.180)	
Epoch: [3][60/188]	Time 0.075 (0.075)	Data 0.000 (0.004)	Loss 0.7611 (0.6402)	Prec@1 68.750 (71.977)	
Epoch: [3][80/188]	Time 0.069 (0.074)	Data 0.000 (0.003)	Loss 0.6146 (0.6433)	Prec@1 78.125 (71.489)	
Epoch: [3][100/188]	Time 0.069 (0.073)	Data 0.000 (0.002)	Loss 0.6460 (0.6464)	Prec@1 75.000 (71.566)	
Epoch: [3][120/188]	Time 0.080 (0.073)	Data 0.000 (0.002)	Loss 0.5397 (0.6427)	Prec@1 81.250 (72.133)	
Epoch: [3][140/188]	Time 0.074 (0.073)	Data 0.000 (0.002)	Loss 0.7746 (0.6387)	Prec@1 62.500 (72.407)	
Epoch: [3][160/188]	Time 0.072 (0.073)	Data 0.000 (0.001)	Loss 0.5043 (0.6392)	Prec@1 81.250 (72.321)	
Epoch: [3][180/188]	Time 0.069 (0.073)	Data 0.000 (0.001)	Loss 0.5369 (0.6368)	Prec@1 78.125 (72.255)	
Test: [0/8]	Time 0.246 (0.246)	Loss 0.4826 (0.4826)	Prec@1 78.125 (78.125)	
 * Prec@1 87.611
Epoch: [4][0/188]	Time 0.296 (0.296)	Data 0.213 (0.213)	Loss 0.5990 (0.5990)	Prec@1 78.125 (78.125)	
Epoch: [4][20/188]	Time 0.070 (0.083)	Data 0.000 (0.010)	Loss 0.5629 (0.6304)	Prec@1 78.125 (74.554)	
Epoch: [4][40/188]	Time 0.066 (0.076)	Data 0.000 (0.005)	Loss 0.7123 (0.6466)	Prec@1 65.625 (72.332)	
Epoch: [4][60/188]	Time 0.064 (0.072)	Data 0.000 (0.004)	Loss 0.5596 (0.6356)	Prec@1 78.125 (72.387)	
Epoch: [4][80/188]	Time 0.061 (0.071)	Data 0.000 (0.003)	Loss 0.4906 (0.6285)	Prec@1 78.125 (72.647)	
Epoch: [4][100/188]	Time 0.074 (0.071)	Data 0.000 (0.002)	Loss 0.5263 (0.6205)	Prec@1 71.875 (73.267)	
Epoch: [4][120/188]	Time 0.070 (0.071)	Data 0.000 (0.002)	Loss 0.4492 (0.6182)	Prec@1 81.250 (73.683)	
Epoch: [4][140/188]	Time 0.067 (0.070)	Data 0.000 (0.002)	Loss 0.5859 (0.6076)	Prec@1 78.125 (74.202)	
Epoch: [4][160/188]	Time 0.069 (0.071)	Data 0.000 (0.001)	Loss 0.6326 (0.6075)	Prec@1 65.625 (74.301)	
Epoch: [4][180/188]	Time 0.069 (0.071)	Data 0.000 (0.001)	Loss 0.4902 (0.6030)	Prec@1 84.375 (74.620)	
Test: [0/8]	Time 0.268 (0.268)	Loss 0.8484 (0.8484)	Prec@1 78.125 (78.125)	
 * Prec@1 92.920
Epoch: [5][0/188]	Time 0.318 (0.318)	Data 0.243 (0.243)	Loss 0.4525 (0.4525)	Prec@1 81.250 (81.250)	
Epoch: [5][20/188]	Time 0.072 (0.082)	Data 0.000 (0.012)	Loss 0.9512 (0.5621)	Prec@1 68.750 (76.190)	
Epoch: [5][40/188]	Time 0.072 (0.077)	Data 0.000 (0.006)	Loss 0.5743 (0.5990)	Prec@1 71.875 (74.619)	
Epoch: [5][60/188]	Time 0.071 (0.076)	Data 0.000 (0.004)	Loss 0.6769 (0.6005)	Prec@1 68.750 (74.436)	
Epoch: [5][80/188]	Time 0.072 (0.075)	Data 0.000 (0.003)	Loss 0.7255 (0.6113)	Prec@1 71.875 (74.074)	
Epoch: [5][100/188]	Time 0.072 (0.074)	Data 0.000 (0.003)	Loss 0.6571 (0.6129)	Prec@1 75.000 (73.948)	
Epoch: [5][120/188]	Time 0.080 (0.074)	Data 0.000 (0.002)	Loss 0.4300 (0.6185)	Prec@1 87.500 (73.709)	
Epoch: [5][140/188]	Time 0.065 (0.073)	Data 0.000 (0.002)	Loss 0.6426 (0.6075)	Prec@1 71.875 (73.958)	
Epoch: [5][160/188]	Time 0.073 (0.073)	Data 0.000 (0.002)	Loss 0.5383 (0.5965)	Prec@1 78.125 (74.495)	
Epoch: [5][180/188]	Time 0.069 (0.073)	Data 0.000 (0.001)	Loss 0.5066 (0.5898)	Prec@1 84.375 (75.000)	
Test: [0/8]	Time 0.252 (0.252)	Loss 0.4297 (0.4297)	Prec@1 87.500 (87.500)	
 * Prec@1 84.071
Epoch: [6][0/188]	Time 0.304 (0.304)	Data 0.228 (0.228)	Loss 0.5159 (0.5159)	Prec@1 81.250 (81.250)	
Epoch: [6][20/188]	Time 0.069 (0.083)	Data 0.000 (0.011)	Loss 0.4166 (0.5394)	Prec@1 78.125 (77.530)	
Epoch: [6][40/188]	Time 0.069 (0.078)	Data 0.000 (0.006)	Loss 0.5394 (0.5379)	Prec@1 78.125 (77.515)	
Epoch: [6][60/188]	Time 0.069 (0.076)	Data 0.000 (0.004)	Loss 0.4505 (0.5506)	Prec@1 87.500 (76.895)	
Epoch: [6][80/188]	Time 0.066 (0.074)	Data 0.000 (0.003)	Loss 0.3915 (0.5362)	Prec@1 81.250 (77.585)	
Epoch: [6][100/188]	Time 0.071 (0.074)	Data 0.000 (0.002)	Loss 0.5499 (0.5306)	Prec@1 81.250 (77.877)	
Epoch: [6][120/188]	Time 0.073 (0.073)	Data 0.000 (0.002)	Loss 0.5655 (0.5492)	Prec@1 71.875 (76.679)	
Epoch: [6][140/188]	Time 0.067 (0.073)	Data 0.000 (0.002)	Loss 0.3687 (0.5489)	Prec@1 81.250 (76.618)	
Epoch: [6][160/188]	Time 0.066 (0.073)	Data 0.000 (0.002)	Loss 0.5233 (0.5431)	Prec@1 84.375 (77.213)	
Epoch: [6][180/188]	Time 0.061 (0.072)	Data 0.000 (0.001)	Loss 0.3632 (0.5433)	Prec@1 87.500 (77.055)	
Test: [0/8]	Time 0.233 (0.233)	Loss 0.3122 (0.3122)	Prec@1 93.750 (93.750)	
 * Prec@1 89.381
Epoch: [7][0/188]	Time 0.295 (0.295)	Data 0.224 (0.224)	Loss 0.5962 (0.5962)	Prec@1 71.875 (71.875)	
Epoch: [7][20/188]	Time 0.065 (0.077)	Data 0.000 (0.011)	Loss 0.3838 (0.4998)	Prec@1 84.375 (77.679)	
Epoch: [7][40/188]	Time 0.073 (0.072)	Data 0.000 (0.006)	Loss 0.5040 (0.5179)	Prec@1 78.125 (77.287)	
Epoch: [7][60/188]	Time 0.069 (0.072)	Data 0.000 (0.004)	Loss 0.3605 (0.5201)	Prec@1 87.500 (77.510)	
Epoch: [7][80/188]	Time 0.071 (0.071)	Data 0.000 (0.003)	Loss 0.5421 (0.5179)	Prec@1 75.000 (77.778)	
Epoch: [7][100/188]	Time 0.067 (0.072)	Data 0.000 (0.002)	Loss 0.5993 (0.5162)	Prec@1 75.000 (77.785)	
Epoch: [7][120/188]	Time 0.070 (0.072)	Data 0.000 (0.002)	Loss 0.7022 (0.5135)	Prec@1 75.000 (78.048)	
Epoch: [7][140/188]	Time 0.076 (0.072)	Data 0.000 (0.002)	Loss 0.4995 (0.5124)	Prec@1 81.250 (78.103)	
Epoch: [7][160/188]	Time 0.076 (0.072)	Data 0.000 (0.002)	Loss 0.7063 (0.5186)	Prec@1 65.625 (77.950)	
Epoch: [7][180/188]	Time 0.068 (0.072)	Data 0.000 (0.001)	Loss 0.2213 (0.5159)	Prec@1 96.875 (78.039)	
Test: [0/8]	Time 0.264 (0.264)	Loss 0.2475 (0.2475)	Prec@1 84.375 (84.375)	
 * Prec@1 92.920
Epoch: [8][0/188]	Time 0.305 (0.305)	Data 0.234 (0.234)	Loss 0.5460 (0.5460)	Prec@1 81.250 (81.250)	
Epoch: [8][20/188]	Time 0.069 (0.083)	Data 0.000 (0.011)	Loss 0.4621 (0.5275)	Prec@1 81.250 (77.976)	
Epoch: [8][40/188]	Time 0.067 (0.079)	Data 0.000 (0.006)	Loss 0.5459 (0.5248)	Prec@1 78.125 (77.973)	
Epoch: [8][60/188]	Time 0.076 (0.077)	Data 0.000 (0.004)	Loss 0.5014 (0.5234)	Prec@1 71.875 (78.125)	
Epoch: [8][80/188]	Time 0.069 (0.075)	Data 0.000 (0.003)	Loss 0.4102 (0.5213)	Prec@1 84.375 (77.971)	
Epoch: [8][100/188]	Time 0.069 (0.074)	Data 0.000 (0.002)	Loss 0.4453 (0.5144)	Prec@1 87.500 (78.403)	
Epoch: [8][120/188]	Time 0.073 (0.073)	Data 0.000 (0.002)	Loss 0.5909 (0.5215)	Prec@1 71.875 (78.202)	
Epoch: [8][140/188]	Time 0.068 (0.073)	Data 0.000 (0.002)	Loss 0.5761 (0.5169)	Prec@1 68.750 (78.280)	
Epoch: [8][160/188]	Time 0.065 (0.073)	Data 0.000 (0.002)	Loss 0.3698 (0.5156)	Prec@1 84.375 (78.183)	
Epoch: [8][180/188]	Time 0.071 (0.073)	Data 0.000 (0.001)	Loss 0.3663 (0.5147)	Prec@1 90.625 (78.160)	
Test: [0/8]	Time 0.255 (0.255)	Loss 0.4369 (0.4369)	Prec@1 81.250 (81.250)	
 * Prec@1 91.150
Epoch: [9][0/188]	Time 0.289 (0.289)	Data 0.214 (0.214)	Loss 0.7943 (0.7943)	Prec@1 68.750 (68.750)	
Epoch: [9][20/188]	Time 0.069 (0.077)	Data 0.000 (0.010)	Loss 0.5377 (0.5178)	Prec@1 78.125 (77.679)	
Epoch: [9][40/188]	Time 0.074 (0.074)	Data 0.000 (0.005)	Loss 0.6356 (0.5123)	Prec@1 68.750 (78.125)	
Epoch: [9][60/188]	Time 0.073 (0.073)	Data 0.000 (0.004)	Loss 0.6823 (0.5307)	Prec@1 68.750 (77.869)	
Epoch: [9][80/188]	Time 0.072 (0.073)	Data 0.000 (0.003)	Loss 0.3151 (0.5160)	Prec@1 87.500 (78.549)	
Epoch: [9][100/188]	Time 0.072 (0.073)	Data 0.000 (0.002)	Loss 0.4791 (0.5125)	Prec@1 75.000 (79.053)	
Epoch: [9][120/188]	Time 0.070 (0.073)	Data 0.000 (0.002)	Loss 0.3985 (0.5107)	Prec@1 78.125 (79.184)	
Epoch: [9][140/188]	Time 0.067 (0.073)	Data 0.000 (0.002)	Loss 0.3134 (0.5055)	Prec@1 90.625 (79.676)	
Epoch: [9][160/188]	Time 0.070 (0.073)	Data 0.000 (0.001)	Loss 0.4428 (0.4966)	Prec@1 84.375 (80.027)	
Epoch: [9][180/188]	Time 0.073 (0.073)	Data 0.000 (0.001)	Loss 0.3218 (0.4987)	Prec@1 90.625 (79.800)	
Test: [0/8]	Time 0.259 (0.259)	Loss 0.2483 (0.2483)	Prec@1 84.375 (84.375)	
 * Prec@1 91.593
Epoch: [10][0/188]	Time 0.314 (0.314)	Data 0.242 (0.242)	Loss 0.4588 (0.4588)	Prec@1 78.125 (78.125)	
Epoch: [10][20/188]	Time 0.076 (0.078)	Data 0.000 (0.012)	Loss 0.5471 (0.4419)	Prec@1 75.000 (82.440)	
Epoch: [10][40/188]	Time 0.072 (0.075)	Data 0.000 (0.006)	Loss 0.3745 (0.4578)	Prec@1 90.625 (81.707)	
Epoch: [10][60/188]	Time 0.073 (0.074)	Data 0.000 (0.004)	Loss 0.5065 (0.4491)	Prec@1 75.000 (82.889)	
Epoch: [10][80/188]	Time 0.071 (0.074)	Data 0.000 (0.003)	Loss 0.4033 (0.4413)	Prec@1 87.500 (83.256)	
Epoch: [10][100/188]	Time 0.071 (0.073)	Data 0.000 (0.003)	Loss 0.4581 (0.4427)	Prec@1 81.250 (83.416)	
Epoch: [10][120/188]	Time 0.070 (0.073)	Data 0.000 (0.002)	Loss 0.5683 (0.4469)	Prec@1 81.250 (83.032)	
Epoch: [10][140/188]	Time 0.075 (0.072)	Data 0.000 (0.002)	Loss 0.5195 (0.4493)	Prec@1 84.375 (82.824)	
Epoch: [10][160/188]	Time 0.077 (0.072)	Data 0.000 (0.002)	Loss 0.5003 (0.4461)	Prec@1 78.125 (82.822)	
Epoch: [10][180/188]	Time 0.067 (0.072)	Data 0.000 (0.001)	Loss 0.3820 (0.4424)	Prec@1 90.625 (82.890)	
Test: [0/8]	Time 0.267 (0.267)	Loss 0.2095 (0.2095)	Prec@1 90.625 (90.625)	
 * Prec@1 90.265
Epoch: [11][0/188]	Time 0.315 (0.315)	Data 0.240 (0.240)	Loss 0.3454 (0.3454)	Prec@1 87.500 (87.500)	
Epoch: [11][20/188]	Time 0.087 (0.083)	Data 0.000 (0.012)	Loss 0.3274 (0.4587)	Prec@1 90.625 (82.292)	
Epoch: [11][40/188]	Time 0.069 (0.077)	Data 0.000 (0.006)	Loss 0.3211 (0.4581)	Prec@1 84.375 (82.165)	
Epoch: [11][60/188]	Time 0.069 (0.076)	Data 0.000 (0.004)	Loss 0.5791 (0.4605)	Prec@1 75.000 (81.814)	
Epoch: [11][80/188]	Time 0.072 (0.075)	Data 0.000 (0.003)	Loss 0.2180 (0.4458)	Prec@1 96.875 (82.562)	
Epoch: [11][100/188]	Time 0.071 (0.074)	Data 0.000 (0.003)	Loss 0.4857 (0.4398)	Prec@1 87.500 (82.890)	
Epoch: [11][120/188]	Time 0.069 (0.074)	Data 0.000 (0.002)	Loss 0.5955 (0.4386)	Prec@1 71.875 (82.877)	
Epoch: [11][140/188]	Time 0.069 (0.073)	Data 0.000 (0.002)	Loss 0.3535 (0.4349)	Prec@1 87.500 (82.757)	
Epoch: [11][160/188]	Time 0.067 (0.073)	Data 0.000 (0.002)	Loss 0.3346 (0.4314)	Prec@1 93.750 (83.094)	
Epoch: [11][180/188]	Time 0.073 (0.073)	Data 0.000 (0.001)	Loss 0.6501 (0.4284)	Prec@1 75.000 (83.235)	
Test: [0/8]	Time 0.250 (0.250)	Loss 0.2018 (0.2018)	Prec@1 90.625 (90.625)	
 * Prec@1 91.150
Epoch: [12][0/188]	Time 0.325 (0.325)	Data 0.242 (0.242)	Loss 0.4820 (0.4820)	Prec@1 75.000 (75.000)	
Epoch: [12][20/188]	Time 0.072 (0.084)	Data 0.000 (0.012)	Loss 0.3064 (0.3944)	Prec@1 90.625 (85.119)	
Epoch: [12][40/188]	Time 0.083 (0.078)	Data 0.000 (0.006)	Loss 0.2883 (0.4007)	Prec@1 93.750 (84.909)	
Epoch: [12][60/188]	Time 0.076 (0.077)	Data 0.000 (0.004)	Loss 0.3663 (0.4094)	Prec@1 87.500 (84.990)	
Epoch: [12][80/188]	Time 0.071 (0.076)	Data 0.000 (0.003)	Loss 0.4947 (0.4090)	Prec@1 78.125 (84.684)	
Epoch: [12][100/188]	Time 0.073 (0.075)	Data 0.000 (0.003)	Loss 0.4475 (0.4185)	Prec@1 84.375 (83.973)	
Epoch: [12][120/188]	Time 0.069 (0.074)	Data 0.000 (0.002)	Loss 0.2432 (0.4167)	Prec@1 93.750 (83.936)	
Epoch: [12][140/188]	Time 0.075 (0.074)	Data 0.000 (0.002)	Loss 0.3467 (0.4195)	Prec@1 84.375 (83.732)	
Epoch: [12][160/188]	Time 0.073 (0.074)	Data 0.000 (0.002)	Loss 0.2844 (0.4235)	Prec@1 87.500 (83.405)	
Epoch: [12][180/188]	Time 0.070 (0.073)	Data 0.000 (0.001)	Loss 0.4452 (0.4228)	Prec@1 81.250 (83.270)	
Test: [0/8]	Time 0.254 (0.254)	Loss 0.1825 (0.1825)	Prec@1 93.750 (93.750)	
 * Prec@1 91.593
Epoch: [13][0/188]	Time 0.316 (0.316)	Data 0.229 (0.229)	Loss 0.5957 (0.5957)	Prec@1 71.875 (71.875)	
Epoch: [13][20/188]	Time 0.073 (0.084)	Data 0.000 (0.011)	Loss 0.3883 (0.4220)	Prec@1 81.250 (82.143)	
Epoch: [13][40/188]	Time 0.074 (0.078)	Data 0.000 (0.006)	Loss 0.2821 (0.4235)	Prec@1 90.625 (82.622)	
Epoch: [13][60/188]	Time 0.063 (0.076)	Data 0.000 (0.004)	Loss 0.3260 (0.4227)	Prec@1 84.375 (82.736)	
Epoch: [13][80/188]	Time 0.065 (0.073)	Data 0.000 (0.003)	Loss 0.3414 (0.4165)	Prec@1 90.625 (83.102)	
Epoch: [13][100/188]	Time 0.065 (0.072)	Data 0.000 (0.002)	Loss 0.4185 (0.4198)	Prec@1 90.625 (82.828)	
Epoch: [13][120/188]	Time 0.073 (0.071)	Data 0.000 (0.002)	Loss 0.4136 (0.4131)	Prec@1 81.250 (83.213)	
Epoch: [13][140/188]	Time 0.070 (0.071)	Data 0.000 (0.002)	Loss 0.3941 (0.4145)	Prec@1 84.375 (83.023)	
Epoch: [13][160/188]	Time 0.073 (0.071)	Data 0.000 (0.002)	Loss 0.4472 (0.4112)	Prec@1 81.250 (83.346)	
Epoch: [13][180/188]	Time 0.070 (0.071)	Data 0.000 (0.001)	Loss 0.3506 (0.4116)	Prec@1 90.625 (83.460)	
Test: [0/8]	Time 0.251 (0.251)	Loss 0.1444 (0.1444)	Prec@1 96.875 (96.875)	
 * Prec@1 89.381
Epoch: [14][0/188]	Time 0.282 (0.282)	Data 0.204 (0.204)	Loss 0.3200 (0.3200)	Prec@1 90.625 (90.625)	
Epoch: [14][20/188]	Time 0.067 (0.082)	Data 0.000 (0.010)	Loss 0.4344 (0.3841)	Prec@1 78.125 (85.268)	
Epoch: [14][40/188]	Time 0.069 (0.076)	Data 0.000 (0.005)	Loss 0.5893 (0.4160)	Prec@1 81.250 (84.146)	
Epoch: [14][60/188]	Time 0.071 (0.075)	Data 0.000 (0.003)	Loss 0.4627 (0.4170)	Prec@1 81.250 (83.811)	
Epoch: [14][80/188]	Time 0.072 (0.074)	Data 0.000 (0.003)	Loss 0.4310 (0.4092)	Prec@1 78.125 (83.873)	
Epoch: [14][100/188]	Time 0.071 (0.073)	Data 0.000 (0.002)	Loss 0.3879 (0.4085)	Prec@1 84.375 (83.632)	
Epoch: [14][120/188]	Time 0.077 (0.073)	Data 0.000 (0.002)	Loss 0.3765 (0.4061)	Prec@1 84.375 (83.678)	
Epoch: [14][140/188]	Time 0.076 (0.073)	Data 0.000 (0.002)	Loss 0.3978 (0.4141)	Prec@1 90.625 (83.223)	
Epoch: [14][160/188]	Time 0.074 (0.073)	Data 0.000 (0.001)	Loss 0.4144 (0.4119)	Prec@1 84.375 (83.424)	
Epoch: [14][180/188]	Time 0.067 (0.073)	Data 0.000 (0.001)	Loss 0.3663 (0.4105)	Prec@1 84.375 (83.633)	
Test: [0/8]	Time 0.249 (0.249)	Loss 0.1431 (0.1431)	Prec@1 93.750 (93.750)	
 * Prec@1 91.593
Epoch: [15][0/188]	Time 0.306 (0.306)	Data 0.233 (0.233)	Loss 0.3775 (0.3775)	Prec@1 87.500 (87.500)	
Epoch: [15][20/188]	Time 0.070 (0.084)	Data 0.000 (0.011)	Loss 0.4707 (0.4421)	Prec@1 87.500 (83.482)	
Epoch: [15][40/188]	Time 0.075 (0.078)	Data 0.000 (0.006)	Loss 0.3769 (0.4435)	Prec@1 78.125 (81.936)	
Epoch: [15][60/188]	Time 0.076 (0.076)	Data 0.000 (0.004)	Loss 0.2648 (0.4299)	Prec@1 93.750 (83.043)	
Epoch: [15][80/188]	Time 0.070 (0.074)	Data 0.000 (0.003)	Loss 0.3985 (0.4258)	Prec@1 90.625 (83.526)	
Epoch: [15][100/188]	Time 0.068 (0.074)	Data 0.000 (0.002)	Loss 0.4272 (0.4197)	Prec@1 87.500 (83.880)	
Epoch: [15][120/188]	Time 0.071 (0.073)	Data 0.000 (0.002)	Loss 0.6197 (0.4167)	Prec@1 71.875 (84.143)	
Epoch: [15][140/188]	Time 0.060 (0.072)	Data 0.000 (0.002)	Loss 0.4744 (0.4197)	Prec@1 87.500 (83.799)	
Epoch: [15][160/188]	Time 0.067 (0.072)	Data 0.000 (0.002)	Loss 0.5474 (0.4188)	Prec@1 78.125 (83.715)	
Epoch: [15][180/188]	Time 0.065 (0.071)	Data 0.000 (0.001)	Loss 0.4948 (0.4177)	Prec@1 75.000 (83.753)	
Test: [0/8]	Time 0.251 (0.251)	Loss 0.1325 (0.1325)	Prec@1 96.875 (96.875)	
 * Prec@1 91.593
Epoch: [16][0/188]	Time 0.310 (0.310)	Data 0.232 (0.232)	Loss 0.4564 (0.4564)	Prec@1 78.125 (78.125)	
Epoch: [16][20/188]	Time 0.077 (0.084)	Data 0.000 (0.011)	Loss 0.3555 (0.3827)	Prec@1 87.500 (84.821)	
Epoch: [16][40/188]	Time 0.068 (0.078)	Data 0.000 (0.006)	Loss 0.4660 (0.4077)	Prec@1 81.250 (84.070)	
Epoch: [16][60/188]	Time 0.070 (0.076)	Data 0.000 (0.004)	Loss 0.2751 (0.4078)	Prec@1 90.625 (84.477)	
Epoch: [16][80/188]	Time 0.067 (0.075)	Data 0.000 (0.003)	Loss 0.5287 (0.4116)	Prec@1 68.750 (84.221)	
Epoch: [16][100/188]	Time 0.068 (0.074)	Data 0.000 (0.002)	Loss 0.4440 (0.4088)	Prec@1 81.250 (84.158)	
Epoch: [16][120/188]	Time 0.073 (0.073)	Data 0.000 (0.002)	Loss 0.5547 (0.4055)	Prec@1 75.000 (84.298)	
Epoch: [16][140/188]	Time 0.070 (0.073)	Data 0.000 (0.002)	Loss 0.3239 (0.4041)	Prec@1 90.625 (84.309)	
Epoch: [16][160/188]	Time 0.063 (0.073)	Data 0.000 (0.002)	Loss 0.4479 (0.4082)	Prec@1 75.000 (84.181)	
Epoch: [16][180/188]	Time 0.066 (0.072)	Data 0.000 (0.001)	Loss 0.4479 (0.4149)	Prec@1 81.250 (83.788)	
Test: [0/8]	Time 0.238 (0.238)	Loss 0.1217 (0.1217)	Prec@1 100.000 (100.000)	
 * Prec@1 90.708
Epoch: [17][0/188]	Time 0.324 (0.324)	Data 0.256 (0.256)	Loss 0.4106 (0.4106)	Prec@1 81.250 (81.250)	
Epoch: [17][20/188]	Time 0.073 (0.082)	Data 0.000 (0.012)	Loss 0.3588 (0.4540)	Prec@1 84.375 (81.399)	
Epoch: [17][40/188]	Time 0.072 (0.076)	Data 0.000 (0.006)	Loss 0.3614 (0.4382)	Prec@1 87.500 (82.470)	
Epoch: [17][60/188]	Time 0.070 (0.075)	Data 0.000 (0.004)	Loss 0.6963 (0.4295)	Prec@1 68.750 (82.736)	
Epoch: [17][80/188]	Time 0.069 (0.074)	Data 0.000 (0.003)	Loss 0.5600 (0.4171)	Prec@1 68.750 (83.449)	
Epoch: [17][100/188]	Time 0.072 (0.074)	Data 0.000 (0.003)	Loss 0.4337 (0.4073)	Prec@1 87.500 (84.127)	
Epoch: [17][120/188]	Time 0.073 (0.073)	Data 0.000 (0.002)	Loss 0.3389 (0.4076)	Prec@1 90.625 (84.246)	
Epoch: [17][140/188]	Time 0.071 (0.073)	Data 0.000 (0.002)	Loss 0.3339 (0.4054)	Prec@1 90.625 (84.353)	
Epoch: [17][160/188]	Time 0.070 (0.073)	Data 0.000 (0.002)	Loss 0.3388 (0.4015)	Prec@1 87.500 (84.511)	
Epoch: [17][180/188]	Time 0.068 (0.073)	Data 0.000 (0.002)	Loss 0.4205 (0.4008)	Prec@1 84.375 (84.599)	
Test: [0/8]	Time 0.249 (0.249)	Loss 0.1347 (0.1347)	Prec@1 100.000 (100.000)	
 * Prec@1 92.035
Epoch: [18][0/188]	Time 0.304 (0.304)	Data 0.229 (0.229)	Loss 0.3778 (0.3778)	Prec@1 87.500 (87.500)	
Epoch: [18][20/188]	Time 0.071 (0.082)	Data 0.000 (0.011)	Loss 0.5669 (0.4266)	Prec@1 71.875 (82.143)	
Epoch: [18][40/188]	Time 0.068 (0.076)	Data 0.000 (0.006)	Loss 0.5022 (0.4174)	Prec@1 78.125 (82.927)	
Epoch: [18][60/188]	Time 0.075 (0.074)	Data 0.000 (0.004)	Loss 0.3033 (0.4038)	Prec@1 87.500 (83.504)	
Epoch: [18][80/188]	Time 0.067 (0.073)	Data 0.000 (0.003)	Loss 0.4684 (0.4087)	Prec@1 81.250 (83.488)	
Epoch: [18][100/188]	Time 0.070 (0.073)	Data 0.000 (0.002)	Loss 0.4400 (0.4070)	Prec@1 84.375 (83.478)	
Epoch: [18][120/188]	Time 0.073 (0.073)	Data 0.000 (0.002)	Loss 0.4877 (0.4072)	Prec@1 81.250 (83.652)	
Epoch: [18][140/188]	Time 0.070 (0.073)	Data 0.000 (0.002)	Loss 0.4002 (0.4038)	Prec@1 84.375 (83.799)	
Epoch: [18][160/188]	Time 0.080 (0.073)	Data 0.000 (0.002)	Loss 0.4629 (0.4041)	Prec@1 81.250 (83.754)	
Epoch: [18][180/188]	Time 0.069 (0.073)	Data 0.000 (0.001)	Loss 0.2521 (0.4026)	Prec@1 93.750 (83.771)	
Test: [0/8]	Time 0.245 (0.245)	Loss 0.1329 (0.1329)	Prec@1 96.875 (96.875)	
 * Prec@1 92.478
Epoch: [19][0/188]	Time 0.321 (0.321)	Data 0.246 (0.246)	Loss 0.2721 (0.2721)	Prec@1 93.750 (93.750)	
Epoch: [19][20/188]	Time 0.071 (0.083)	Data 0.000 (0.012)	Loss 0.4255 (0.4297)	Prec@1 84.375 (84.226)	
Epoch: [19][40/188]	Time 0.068 (0.077)	Data 0.000 (0.006)	Loss 0.3907 (0.4119)	Prec@1 81.250 (84.680)	
Epoch: [19][60/188]	Time 0.066 (0.074)	Data 0.000 (0.004)	Loss 0.4820 (0.4061)	Prec@1 78.125 (85.195)	
Epoch: [19][80/188]	Time 0.071 (0.073)	Data 0.000 (0.003)	Loss 0.4233 (0.4078)	Prec@1 87.500 (84.799)	
Epoch: [19][100/188]	Time 0.075 (0.073)	Data 0.000 (0.003)	Loss 0.2290 (0.3991)	Prec@1 93.750 (85.025)	
Epoch: [19][120/188]	Time 0.074 (0.073)	Data 0.000 (0.002)	Loss 0.4743 (0.3987)	Prec@1 87.500 (85.072)	
Epoch: [19][140/188]	Time 0.070 (0.073)	Data 0.000 (0.002)	Loss 0.5178 (0.3962)	Prec@1 75.000 (84.907)	
Epoch: [19][160/188]	Time 0.068 (0.072)	Data 0.000 (0.002)	Loss 0.3796 (0.3957)	Prec@1 84.375 (84.724)	
Epoch: [19][180/188]	Time 0.067 (0.072)	Data 0.000 (0.001)	Loss 0.5232 (0.3993)	Prec@1 78.125 (84.599)	
Test: [0/8]	Time 0.254 (0.254)	Loss 0.1419 (0.1419)	Prec@1 96.875 (96.875)	
 * Prec@1 93.805
Epoch: [20][0/188]	Time 0.276 (0.276)	Data 0.198 (0.198)	Loss 0.5610 (0.5610)	Prec@1 78.125 (78.125)	
Epoch: [20][20/188]	Time 0.076 (0.081)	Data 0.000 (0.010)	Loss 0.2979 (0.4067)	Prec@1 90.625 (82.738)	
Epoch: [20][40/188]	Time 0.072 (0.076)	Data 0.000 (0.005)	Loss 0.4016 (0.4137)	Prec@1 84.375 (83.765)	
Epoch: [20][60/188]	Time 0.072 (0.074)	Data 0.000 (0.003)	Loss 0.6728 (0.4073)	Prec@1 75.000 (84.477)	
Epoch: [20][80/188]	Time 0.074 (0.073)	Data 0.000 (0.003)	Loss 0.4220 (0.4064)	Prec@1 81.250 (84.568)	
Epoch: [20][100/188]	Time 0.074 (0.073)	Data 0.000 (0.002)	Loss 0.3373 (0.4100)	Prec@1 84.375 (84.066)	
Epoch: [20][120/188]	Time 0.072 (0.072)	Data 0.000 (0.002)	Loss 0.3376 (0.4030)	Prec@1 90.625 (84.427)	
Epoch: [20][140/188]	Time 0.070 (0.072)	Data 0.000 (0.002)	Loss 0.4710 (0.4030)	Prec@1 84.375 (84.441)	
Epoch: [20][160/188]	Time 0.070 (0.072)	Data 0.000 (0.001)	Loss 0.3898 (0.3972)	Prec@1 84.375 (84.763)	
Epoch: [20][180/188]	Time 0.067 (0.072)	Data 0.000 (0.001)	Loss 0.5373 (0.3991)	Prec@1 81.250 (84.720)	
Test: [0/8]	Time 0.251 (0.251)	Loss 0.1543 (0.1543)	Prec@1 96.875 (96.875)	
 * Prec@1 92.035
Epoch: [21][0/188]	Time 0.272 (0.272)	Data 0.195 (0.195)	Loss 0.3051 (0.3051)	Prec@1 93.750 (93.750)	
Epoch: [21][20/188]	Time 0.070 (0.081)	Data 0.000 (0.009)	Loss 0.4188 (0.4156)	Prec@1 87.500 (83.929)	
Epoch: [21][40/188]	Time 0.074 (0.076)	Data 0.000 (0.005)	Loss 0.4143 (0.4100)	Prec@1 81.250 (84.146)	
Epoch: [21][60/188]	Time 0.071 (0.075)	Data 0.000 (0.003)	Loss 0.3190 (0.4067)	Prec@1 90.625 (83.965)	
Epoch: [21][80/188]	Time 0.073 (0.074)	Data 0.000 (0.003)	Loss 0.3627 (0.3968)	Prec@1 87.500 (84.491)	
Epoch: [21][100/188]	Time 0.072 (0.073)	Data 0.000 (0.002)	Loss 0.3904 (0.4030)	Prec@1 87.500 (83.818)	
Epoch: [21][120/188]	Time 0.076 (0.073)	Data 0.000 (0.002)	Loss 0.3334 (0.4030)	Prec@1 84.375 (83.626)	
Epoch: [21][140/188]	Time 0.073 (0.073)	Data 0.000 (0.002)	Loss 0.5143 (0.4001)	Prec@1 78.125 (83.466)	
Epoch: [21][160/188]	Time 0.078 (0.073)	Data 0.000 (0.001)	Loss 0.3061 (0.3999)	Prec@1 87.500 (83.599)	
Epoch: [21][180/188]	Time 0.068 (0.073)	Data 0.000 (0.001)	Loss 0.2952 (0.3972)	Prec@1 87.500 (83.840)	
Test: [0/8]	Time 0.250 (0.250)	Loss 0.1484 (0.1484)	Prec@1 96.875 (96.875)	
 * Prec@1 90.708
Epoch: [22][0/188]	Time 0.319 (0.319)	Data 0.237 (0.237)	Loss 0.2838 (0.2838)	Prec@1 93.750 (93.750)	
Epoch: [22][20/188]	Time 0.076 (0.084)	Data 0.000 (0.011)	Loss 0.3927 (0.4007)	Prec@1 81.250 (83.631)	
Epoch: [22][40/188]	Time 0.071 (0.077)	Data 0.000 (0.006)	Loss 0.4392 (0.3728)	Prec@1 78.125 (85.899)	
Epoch: [22][60/188]	Time 0.071 (0.076)	Data 0.000 (0.004)	Loss 0.4824 (0.3705)	Prec@1 78.125 (85.809)	
Epoch: [22][80/188]	Time 0.070 (0.075)	Data 0.000 (0.003)	Loss 0.2807 (0.3771)	Prec@1 87.500 (85.262)	
Epoch: [22][100/188]	Time 0.069 (0.074)	Data 0.000 (0.002)	Loss 0.2760 (0.3828)	Prec@1 93.750 (84.901)	
Epoch: [22][120/188]	Time 0.066 (0.073)	Data 0.000 (0.002)	Loss 0.5213 (0.3856)	Prec@1 81.250 (84.762)	
Epoch: [22][140/188]	Time 0.073 (0.073)	Data 0.000 (0.002)	Loss 0.5883 (0.3925)	Prec@1 81.250 (84.574)	
Epoch: [22][160/188]	Time 0.068 (0.073)	Data 0.000 (0.002)	Loss 0.4365 (0.3908)	Prec@1 81.250 (84.724)	
Epoch: [22][180/188]	Time 0.070 (0.073)	Data 0.000 (0.001)	Loss 0.4204 (0.3885)	Prec@1 90.625 (84.927)	
Test: [0/8]	Time 0.244 (0.244)	Loss 0.1611 (0.1611)	Prec@1 93.750 (93.750)	
 * Prec@1 92.478
Epoch: [23][0/188]	Time 0.300 (0.300)	Data 0.226 (0.226)	Loss 0.2713 (0.2713)	Prec@1 93.750 (93.750)	
Epoch: [23][20/188]	Time 0.068 (0.083)	Data 0.000 (0.011)	Loss 0.2939 (0.3576)	Prec@1 87.500 (87.202)	
Epoch: [23][40/188]	Time 0.070 (0.077)	Data 0.000 (0.006)	Loss 0.3752 (0.4102)	Prec@1 81.250 (84.375)	
Epoch: [23][60/188]	Time 0.071 (0.075)	Data 0.000 (0.004)	Loss 0.5086 (0.4115)	Prec@1 71.875 (84.119)	
Epoch: [23][80/188]	Time 0.063 (0.074)	Data 0.000 (0.003)	Loss 0.4139 (0.4064)	Prec@1 93.750 (84.645)	
Epoch: [23][100/188]	Time 0.066 (0.072)	Data 0.000 (0.002)	Loss 0.4706 (0.4007)	Prec@1 78.125 (84.623)	
Epoch: [23][120/188]	Time 0.065 (0.071)	Data 0.000 (0.002)	Loss 0.4480 (0.3956)	Prec@1 84.375 (84.762)	
Epoch: [23][140/188]	Time 0.069 (0.071)	Data 0.000 (0.002)	Loss 0.4976 (0.3974)	Prec@1 75.000 (84.641)	
Epoch: [23][160/188]	Time 0.069 (0.071)	Data 0.000 (0.002)	Loss 0.3547 (0.3972)	Prec@1 87.500 (84.530)	
Epoch: [23][180/188]	Time 0.066 (0.071)	Data 0.000 (0.001)	Loss 0.4546 (0.3990)	Prec@1 84.375 (84.479)	
Test: [0/8]	Time 0.255 (0.255)	Loss 0.1541 (0.1541)	Prec@1 93.750 (93.750)	
 * Prec@1 90.708
Epoch: [24][0/188]	Time 0.276 (0.276)	Data 0.202 (0.202)	Loss 0.3634 (0.3634)	Prec@1 87.500 (87.500)	
Epoch: [24][20/188]	Time 0.072 (0.081)	Data 0.000 (0.010)	Loss 0.4539 (0.3967)	Prec@1 81.250 (83.333)	
Epoch: [24][40/188]	Time 0.065 (0.074)	Data 0.000 (0.005)	Loss 0.2690 (0.3958)	Prec@1 90.625 (83.384)	
Epoch: [24][60/188]	Time 0.066 (0.071)	Data 0.000 (0.003)	Loss 0.4795 (0.3945)	Prec@1 78.125 (83.760)	
Epoch: [24][80/188]	Time 0.072 (0.071)	Data 0.000 (0.003)	Loss 0.4868 (0.3992)	Prec@1 84.375 (83.912)	
Epoch: [24][100/188]	Time 0.071 (0.071)	Data 0.000 (0.002)	Loss 0.4738 (0.4022)	Prec@1 78.125 (84.097)	
Epoch: [24][120/188]	Time 0.070 (0.071)	Data 0.000 (0.002)	Loss 0.3325 (0.3976)	Prec@1 84.375 (84.375)	
Epoch: [24][140/188]	Time 0.077 (0.071)	Data 0.000 (0.002)	Loss 0.3696 (0.3973)	Prec@1 87.500 (84.552)	
Epoch: [24][160/188]	Time 0.071 (0.071)	Data 0.000 (0.001)	Loss 0.2484 (0.3944)	Prec@1 93.750 (84.821)	
Epoch: [24][180/188]	Time 0.070 (0.071)	Data 0.000 (0.001)	Loss 0.3034 (0.3946)	Prec@1 93.750 (84.755)	
Test: [0/8]	Time 0.237 (0.237)	Loss 0.1314 (0.1314)	Prec@1 96.875 (96.875)	
 * Prec@1 91.150
Epoch: [25][0/188]	Time 0.304 (0.304)	Data 0.233 (0.233)	Loss 0.3684 (0.3684)	Prec@1 81.250 (81.250)	
Epoch: [25][20/188]	Time 0.066 (0.082)	Data 0.000 (0.011)	Loss 0.3037 (0.3425)	Prec@1 84.375 (86.310)	
Epoch: [25][40/188]	Time 0.071 (0.074)	Data 0.000 (0.006)	Loss 0.3670 (0.3885)	Prec@1 81.250 (84.299)	
Epoch: [25][60/188]	Time 0.071 (0.073)	Data 0.000 (0.004)	Loss 0.5253 (0.3919)	Prec@1 71.875 (84.119)	
Epoch: [25][80/188]	Time 0.068 (0.073)	Data 0.000 (0.003)	Loss 0.4652 (0.3881)	Prec@1 84.375 (84.375)	
Epoch: [25][100/188]	Time 0.065 (0.072)	Data 0.000 (0.002)	Loss 0.3612 (0.3852)	Prec@1 87.500 (84.592)	
Epoch: [25][120/188]	Time 0.066 (0.072)	Data 0.000 (0.002)	Loss 0.3123 (0.3813)	Prec@1 87.500 (84.659)	
Epoch: [25][140/188]	Time 0.071 (0.072)	Data 0.000 (0.002)	Loss 0.3127 (0.3841)	Prec@1 90.625 (84.552)	
Epoch: [25][160/188]	Time 0.074 (0.072)	Data 0.000 (0.002)	Loss 0.7072 (0.3859)	Prec@1 78.125 (84.627)	
Epoch: [25][180/188]	Time 0.068 (0.072)	Data 0.000 (0.001)	Loss 0.4852 (0.3881)	Prec@1 84.375 (84.496)	
Test: [0/8]	Time 0.249 (0.249)	Loss 0.1528 (0.1528)	Prec@1 93.750 (93.750)	
 * Prec@1 91.150
Epoch: [26][0/188]	Time 0.301 (0.301)	Data 0.224 (0.224)	Loss 0.2266 (0.2266)	Prec@1 93.750 (93.750)	
Epoch: [26][20/188]	Time 0.068 (0.081)	Data 0.000 (0.011)	Loss 0.3131 (0.4193)	Prec@1 84.375 (83.482)	
Epoch: [26][40/188]	Time 0.067 (0.075)	Data 0.000 (0.006)	Loss 0.2983 (0.4098)	Prec@1 84.375 (84.070)	
Epoch: [26][60/188]	Time 0.068 (0.073)	Data 0.000 (0.004)	Loss 0.3850 (0.4039)	Prec@1 87.500 (84.529)	
Epoch: [26][80/188]	Time 0.067 (0.072)	Data 0.000 (0.003)	Loss 0.4161 (0.4069)	Prec@1 87.500 (84.452)	
Epoch: [26][100/188]	Time 0.067 (0.071)	Data 0.000 (0.002)	Loss 0.3178 (0.4048)	Prec@1 90.625 (84.561)	
Epoch: [26][120/188]	Time 0.069 (0.071)	Data 0.000 (0.002)	Loss 0.3517 (0.3984)	Prec@1 81.250 (84.737)	
Epoch: [26][140/188]	Time 0.071 (0.071)	Data 0.000 (0.002)	Loss 0.5027 (0.3996)	Prec@1 75.000 (84.707)	
Epoch: [26][160/188]	Time 0.068 (0.071)	Data 0.000 (0.001)	Loss 0.4709 (0.3980)	Prec@1 78.125 (84.705)	
Epoch: [26][180/188]	Time 0.072 (0.071)	Data 0.000 (0.001)	Loss 0.2669 (0.3960)	Prec@1 87.500 (84.789)	
Test: [0/8]	Time 0.246 (0.246)	Loss 0.1370 (0.1370)	Prec@1 96.875 (96.875)	
 * Prec@1 92.920
Epoch: [27][0/188]	Time 0.325 (0.325)	Data 0.250 (0.250)	Loss 0.1607 (0.1607)	Prec@1 100.000 (100.000)	
Epoch: [27][20/188]	Time 0.072 (0.083)	Data 0.000 (0.012)	Loss 0.3128 (0.3730)	Prec@1 84.375 (86.012)	
Epoch: [27][40/188]	Time 0.075 (0.077)	Data 0.000 (0.006)	Loss 0.3857 (0.3826)	Prec@1 81.250 (85.823)	
Epoch: [27][60/188]	Time 0.073 (0.075)	Data 0.000 (0.004)	Loss 0.3507 (0.3967)	Prec@1 93.750 (85.195)	
Epoch: [27][80/188]	Time 0.068 (0.075)	Data 0.000 (0.003)	Loss 0.6025 (0.3999)	Prec@1 78.125 (84.838)	
Epoch: [27][100/188]	Time 0.071 (0.074)	Data 0.000 (0.003)	Loss 0.2593 (0.3981)	Prec@1 93.750 (84.777)	
Epoch: [27][120/188]	Time 0.068 (0.073)	Data 0.000 (0.002)	Loss 0.2815 (0.3916)	Prec@1 87.500 (85.021)	
Epoch: [27][140/188]	Time 0.069 (0.073)	Data 0.000 (0.002)	Loss 0.2362 (0.3901)	Prec@1 93.750 (85.129)	
Epoch: [27][160/188]	Time 0.074 (0.073)	Data 0.000 (0.002)	Loss 0.4881 (0.3910)	Prec@1 75.000 (85.035)	
Epoch: [27][180/188]	Time 0.069 (0.073)	Data 0.000 (0.002)	Loss 0.3167 (0.3900)	Prec@1 90.625 (85.100)	
Test: [0/8]	Time 0.244 (0.244)	Loss 0.1480 (0.1480)	Prec@1 93.750 (93.750)	
 * Prec@1 91.593
Epoch: [28][0/188]	Time 0.351 (0.351)	Data 0.276 (0.276)	Loss 0.3777 (0.3777)	Prec@1 87.500 (87.500)	
Epoch: [28][20/188]	Time 0.069 (0.084)	Data 0.000 (0.013)	Loss 0.4308 (0.4248)	Prec@1 87.500 (83.631)	
Epoch: [28][40/188]	Time 0.065 (0.077)	Data 0.000 (0.007)	Loss 0.3232 (0.4177)	Prec@1 87.500 (83.384)	
Epoch: [28][60/188]	Time 0.072 (0.075)	Data 0.000 (0.005)	Loss 0.2935 (0.4136)	Prec@1 96.875 (83.555)	
Epoch: [28][80/188]	Time 0.075 (0.074)	Data 0.000 (0.004)	Loss 0.3585 (0.4065)	Prec@1 90.625 (83.873)	
Epoch: [28][100/188]	Time 0.066 (0.074)	Data 0.000 (0.003)	Loss 0.1954 (0.3972)	Prec@1 100.000 (84.592)	
Epoch: [28][120/188]	Time 0.071 (0.073)	Data 0.000 (0.002)	Loss 0.4859 (0.3880)	Prec@1 81.250 (85.331)	
Epoch: [28][140/188]	Time 0.066 (0.073)	Data 0.000 (0.002)	Loss 0.3903 (0.3902)	Prec@1 93.750 (85.372)	
Epoch: [28][160/188]	Time 0.069 (0.073)	Data 0.000 (0.002)	Loss 0.3537 (0.3909)	Prec@1 87.500 (85.093)	
Epoch: [28][180/188]	Time 0.064 (0.073)	Data 0.000 (0.002)	Loss 0.5082 (0.3893)	Prec@1 75.000 (85.066)	
Test: [0/8]	Time 0.238 (0.238)	Loss 0.1719 (0.1719)	Prec@1 96.875 (96.875)	
 * Prec@1 90.265
Epoch: [29][0/188]	Time 0.280 (0.280)	Data 0.202 (0.202)	Loss 0.4180 (0.4180)	Prec@1 84.375 (84.375)	
Epoch: [29][20/188]	Time 0.068 (0.081)	Data 0.000 (0.010)	Loss 0.4177 (0.3887)	Prec@1 84.375 (86.161)	
Epoch: [29][40/188]	Time 0.070 (0.076)	Data 0.000 (0.005)	Loss 0.2591 (0.3912)	Prec@1 90.625 (85.899)	
Epoch: [29][60/188]	Time 0.070 (0.075)	Data 0.000 (0.003)	Loss 0.3031 (0.3998)	Prec@1 84.375 (84.426)	
Epoch: [29][80/188]	Time 0.076 (0.074)	Data 0.000 (0.003)	Loss 0.3486 (0.3963)	Prec@1 90.625 (84.221)	
Epoch: [29][100/188]	Time 0.089 (0.074)	Data 0.000 (0.002)	Loss 0.4639 (0.3993)	Prec@1 78.125 (83.911)	
Epoch: [29][120/188]	Time 0.070 (0.073)	Data 0.000 (0.002)	Loss 0.3207 (0.3939)	Prec@1 87.500 (84.375)	
Epoch: [29][140/188]	Time 0.073 (0.072)	Data 0.000 (0.002)	Loss 0.3749 (0.3888)	Prec@1 90.625 (84.752)	
Epoch: [29][160/188]	Time 0.080 (0.072)	Data 0.000 (0.001)	Loss 0.3808 (0.3904)	Prec@1 81.250 (84.666)	
Epoch: [29][180/188]	Time 0.064 (0.072)	Data 0.000 (0.001)	Loss 0.5391 (0.3911)	Prec@1 81.250 (84.634)	
Test: [0/8]	Time 0.268 (0.268)	Loss 0.1472 (0.1472)	Prec@1 93.750 (93.750)	
 * Prec@1 92.035
save path : ./checkpointRFMNet_L3C2
Namespace(arch='rfmnet', batch_size=32, data='../Data_processing_data2/Result_Split_data', epochs=30, evaluate=True, lr=0.001, momentum=0.9, pin_memory=True, print_freq=20, resume='./checkpointRFMNet_L3C2/model_best.pth.tar', save_dir='./checkpointRFMNet_L3C2', start_epoch=0, weight_decay=0.0001, workers=4)
=> creating model 'rfmnet'
=> network :
 RFMNet(
  (conv1): Conv2d(1, 128, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): RFMBlock(
      (conv1): Conv2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
      (downsample): Conv2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)
      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer2): Sequential(
    (0): RFMBlock(
      (conv1): Conv2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
      (downsample): Conv2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False)
      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): RFMBlock(
      (conv1): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): RFMBlock(
      (conv1): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer3): Sequential(
    (0): RFMBlock(
      (conv1): Conv2d(512, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256, bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)
      (downsample): Conv2d(512, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256, bias=False)
      (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): RFMBlock(
      (conv1): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)
      (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): RFMBlock(
      (conv1): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)
      (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(2, 2))
  (fc): Linear(in_features=4096, out_features=3, bias=True)
)
Test: [0/8]	Time 0.542 (0.542)	Loss 0.1419 (0.1419)	Prec@1 96.875 (96.875)	
 * Prec@1 93.805
save path : ./checkpointRFMNet_L3C2
Namespace(arch='rfmnet', batch_size=32, data='../Data_processing_data2/Result_Split_data', epochs=30, evaluate=True, lr=0.001, momentum=0.9, pin_memory=True, print_freq=20, resume='./checkpointRFMNet_L3C2/model_best.pth.tar', save_dir='./checkpointRFMNet_L3C2', start_epoch=0, weight_decay=0.0001, workers=4)
=> creating model 'rfmnet'
=> network :
 RFMNet(
  (conv1): Conv2d(1, 128, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): RFMBlock(
      (conv1): Conv2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
      (downsample): Conv2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)
      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer2): Sequential(
    (0): RFMBlock(
      (conv1): Conv2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
      (downsample): Conv2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False)
      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): RFMBlock(
      (conv1): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): RFMBlock(
      (conv1): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer3): Sequential(
    (0): RFMBlock(
      (conv1): Conv2d(512, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256, bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)
      (downsample): Conv2d(512, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256, bias=False)
      (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): RFMBlock(
      (conv1): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)
      (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): RFMBlock(
      (conv1): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)
      (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(2, 2))
  (fc): Linear(in_features=4096, out_features=3, bias=True)
)
Test: [0/8]	Time 0.534 (0.534)	Loss 0.3068 (0.3068)	Prec@1 87.500 (87.500)	
 * Prec@1 91.150
save path : ./checkpointRFMNet_L3C2
Namespace(arch='rfmnet', batch_size=32, data='../Data_processing_data2/Result_Split_data', epochs=30, evaluate=True, lr=0.001, momentum=0.9, pin_memory=True, print_freq=20, resume='./checkpointRFMNet_L3C2/model_best.pth.tar', save_dir='./checkpointRFMNet_L3C2', start_epoch=0, weight_decay=0.0001, workers=4)
=> creating model 'rfmnet'
=> network :
 RFMNet(
  (conv1): Conv2d(1, 128, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): RFMBlock(
      (conv1): Conv2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
      (downsample): Conv2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)
      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer2): Sequential(
    (0): RFMBlock(
      (conv1): Conv2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
      (downsample): Conv2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False)
      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): RFMBlock(
      (conv1): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): RFMBlock(
      (conv1): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer3): Sequential(
    (0): RFMBlock(
      (conv1): Conv2d(512, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256, bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)
      (downsample): Conv2d(512, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256, bias=False)
      (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): RFMBlock(
      (conv1): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)
      (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): RFMBlock(
      (conv1): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)
      (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(2, 2))
  (fc): Linear(in_features=4096, out_features=3, bias=True)
)
Test: [0/8]	Time 0.549 (0.549)	Loss 0.2244 (0.2244)	Prec@1 93.750 (93.750)	
 * Prec@1 90.265
save path : ./checkpointRFMNet_L3C2
Namespace(arch='rfmnet', batch_size=32, data='../Data_processing_data2/Result_Split_data', epochs=30, evaluate=True, lr=0.001, momentum=0.9, pin_memory=True, print_freq=20, resume='./checkpointRFMNet_L3C2/model_best.pth.tar', save_dir='./checkpointRFMNet_L3C2', start_epoch=0, weight_decay=0.0001, workers=4)
=> creating model 'rfmnet'
=> network :
 RFMNet(
  (conv1): Conv2d(1, 128, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): RFMBlock(
      (conv1): Conv2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
      (downsample): Conv2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)
      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer2): Sequential(
    (0): RFMBlock(
      (conv1): Conv2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
      (downsample): Conv2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False)
      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): RFMBlock(
      (conv1): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): RFMBlock(
      (conv1): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer3): Sequential(
    (0): RFMBlock(
      (conv1): Conv2d(512, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256, bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)
      (downsample): Conv2d(512, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256, bias=False)
      (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): RFMBlock(
      (conv1): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)
      (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): RFMBlock(
      (conv1): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)
      (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(2, 2))
  (fc): Linear(in_features=4096, out_features=3, bias=True)
)
Test: [0/8]	Time 0.544 (0.544)	Loss 0.4090 (0.4090)	Prec@1 87.500 (87.500)	
 * Prec@1 89.823
save path : ./checkpointRFMNet_L3C2
Namespace(arch='rfmnet', batch_size=32, data='../Data_processing_data2/Result_Split_data', epochs=30, evaluate=True, lr=0.001, momentum=0.9, pin_memory=True, print_freq=20, resume='./checkpointRFMNet_L3C2/model_best.pth.tar', save_dir='./checkpointRFMNet_L3C2', start_epoch=0, weight_decay=0.0001, workers=4)
=> creating model 'rfmnet'
=> network :
 RFMNet(
  (conv1): Conv2d(1, 128, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): RFMBlock(
      (conv1): Conv2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
      (downsample): Conv2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)
      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer2): Sequential(
    (0): RFMBlock(
      (conv1): Conv2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
      (downsample): Conv2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False)
      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): RFMBlock(
      (conv1): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): RFMBlock(
      (conv1): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer3): Sequential(
    (0): RFMBlock(
      (conv1): Conv2d(512, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256, bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)
      (downsample): Conv2d(512, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256, bias=False)
      (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): RFMBlock(
      (conv1): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)
      (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): RFMBlock(
      (conv1): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)
      (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(2, 2))
  (fc): Linear(in_features=4096, out_features=3, bias=True)
)
Test: [0/8]	Time 0.495 (0.495)	Loss 0.1782 (0.1782)	Prec@1 93.750 (93.750)	
 * Prec@1 95.133
save path : ./checkpointRFMNet_L3C2
Namespace(arch='rfmnet', batch_size=32, data='../Data_processing_data2/Result_Split_data', epochs=30, evaluate=True, lr=0.001, momentum=0.9, pin_memory=True, print_freq=20, resume='./checkpointRFMNet_L3C2/model_best.pth.tar', save_dir='./checkpointRFMNet_L3C2', start_epoch=0, weight_decay=0.0001, workers=4)
=> creating model 'rfmnet'
=> network :
 RFMNet(
  (conv1): Conv2d(1, 128, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): RFMBlock(
      (conv1): Conv2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
      (downsample): Conv2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)
      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer2): Sequential(
    (0): RFMBlock(
      (conv1): Conv2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
      (downsample): Conv2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False)
      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): RFMBlock(
      (conv1): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): RFMBlock(
      (conv1): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer3): Sequential(
    (0): RFMBlock(
      (conv1): Conv2d(512, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256, bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)
      (downsample): Conv2d(512, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256, bias=False)
      (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): RFMBlock(
      (conv1): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)
      (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): RFMBlock(
      (conv1): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)
      (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(2, 2))
  (fc): Linear(in_features=4096, out_features=3, bias=True)
)
Test: [0/8]	Time 0.584 (0.584)	Loss 0.1782 (0.1782)	Prec@1 93.750 (93.750)	
 * Prec@1 95.133
save path : ./checkpointRFMNet_L3C2
Namespace(arch='rfmnet', batch_size=32, data='../Data_processing_data2/Result_Split_data', epochs=30, evaluate=True, lr=0.001, momentum=0.9, pin_memory=True, print_freq=20, resume='./checkpointRFMNet_L3C2/model_best.pth.tar', save_dir='./checkpointRFMNet_L3C2', start_epoch=0, weight_decay=0.0001, workers=4)
=> creating model 'rfmnet'
=> network :
 RFMNet(
  (conv1): Conv2d(1, 128, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): RFMBlock(
      (conv1): Conv2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
      (downsample): Conv2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)
      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer2): Sequential(
    (0): RFMBlock(
      (conv1): Conv2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
      (downsample): Conv2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False)
      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): RFMBlock(
      (conv1): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): RFMBlock(
      (conv1): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer3): Sequential(
    (0): RFMBlock(
      (conv1): Conv2d(512, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256, bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)
      (downsample): Conv2d(512, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256, bias=False)
      (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): RFMBlock(
      (conv1): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)
      (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): RFMBlock(
      (conv1): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)
      (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(2, 2))
  (fc): Linear(in_features=4096, out_features=3, bias=True)
)
Test: [0/8]	Time 0.673 (0.673)	Loss 0.1782 (0.1782)	Prec@1 93.750 (93.750)	
 * Prec@1 95.133
save path : ./checkpointRFMNet_L3C2
Namespace(arch='rfmnet', batch_size=32, data='../Data_processing_data2/Result_Split_data', epochs=30, evaluate=True, lr=0.001, momentum=0.9, pin_memory=True, print_freq=20, resume='./checkpointRFMNet_L3C2/model_best.pth.tar', save_dir='./checkpointRFMNet_L3C2', start_epoch=0, weight_decay=0.0001, workers=4)
=> creating model 'rfmnet'
=> network :
 RFMNet(
  (conv1): Conv2d(1, 128, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): RFMBlock(
      (conv1): Conv2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
      (downsample): Conv2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)
      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer2): Sequential(
    (0): RFMBlock(
      (conv1): Conv2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
      (downsample): Conv2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False)
      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): RFMBlock(
      (conv1): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): RFMBlock(
      (conv1): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer3): Sequential(
    (0): RFMBlock(
      (conv1): Conv2d(512, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256, bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)
      (downsample): Conv2d(512, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256, bias=False)
      (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): RFMBlock(
      (conv1): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)
      (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): RFMBlock(
      (conv1): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)
      (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(2, 2))
  (fc): Linear(in_features=4096, out_features=3, bias=True)
)
Test: [0/8]	Time 0.644 (0.644)	Loss 0.1782 (0.1782)	Prec@1 93.750 (93.750)	
 * Prec@1 95.133
save path : ./checkpointRFMNet_L3C2
Namespace(arch='rfmnet', batch_size=32, data='../Data_processing_data2/Result_Split_data', epochs=30, evaluate=True, lr=0.001, momentum=0.9, pin_memory=True, print_freq=20, resume='./checkpointRFMNet_L3C2/model_best.pth.tar', save_dir='./checkpointRFMNet_L3C2', start_epoch=0, weight_decay=0.0001, workers=4)
=> creating model 'rfmnet'
=> network :
 RFMNet(
  (conv1): Conv2d(1, 128, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): RFMBlock(
      (conv1): Conv2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
      (downsample): Conv2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)
      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer2): Sequential(
    (0): RFMBlock(
      (conv1): Conv2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
      (downsample): Conv2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False)
      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): RFMBlock(
      (conv1): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): RFMBlock(
      (conv1): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer3): Sequential(
    (0): RFMBlock(
      (conv1): Conv2d(512, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256, bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)
      (downsample): Conv2d(512, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256, bias=False)
      (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): RFMBlock(
      (conv1): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)
      (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): RFMBlock(
      (conv1): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)
      (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(2, 2))
  (fc): Linear(in_features=4096, out_features=3, bias=True)
)
Test: [0/8]	Time 11.326 (11.326)	Loss 0.1782 (0.1782)	Prec@1 93.750 (93.750)	
 * Prec@1 95.133
